"""
crawlers.py - í…Œë§ˆ ë°ì´í„° í¬ë¡¤ë§ ëª¨ë“ˆ

ì´ íŒŒì¼ì€ ë„¤ì´ë²„ ì¦ê¶Œ, í•œêµ­ê²½ì œ ë“±ì—ì„œ í…Œë§ˆ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë„¤ì´ë²„ ì¦ê¶Œ ì¸ê¸° í…Œë§ˆ í¬ë¡¤ë§
- í•œêµ­ê²½ì œ í…Œë§ˆ ì •ë³´ í¬ë¡¤ë§
- í…Œë§ˆë³„ ì¢…ëª© ëª©ë¡ ìˆ˜ì§‘
- ë‰´ìŠ¤ ì–¸ê¸‰ ë¹ˆë„ ìˆ˜ì§‘

ì‚¬ìš©ë²•:
    from modules.theme_analyzer.crawlers import (
        crawl_naver_themes,
        crawl_hankyung_themes,
        crawl_theme_stocks,
        crawl_theme_news_count
    )
    
    naver_themes = crawl_naver_themes()
    hankyung_themes = crawl_hankyung_themes()
"""

import time
import random
from typing import Optional
from datetime import datetime, timedelta

import httpx
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ logger ì‚¬ìš©
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from logger import logger


# ===== ìƒìˆ˜ ì •ì˜ =====
# í¬ë¡¤ë§ ì‹œ ì‚¬ìš©í•  User-Agent (ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ)
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Referer": "https://www.google.com/"
}

# ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ì°¨ë‹¨ ë°©ì§€
MIN_DELAY = 1.0
MAX_DELAY = 2.5


def _random_delay():
    """ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ëŒ€ê¸°"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)


def _safe_float(value: str, default: float = 0.0) -> float:
    """
    ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ floatìœ¼ë¡œ ë³€í™˜
    
    Args:
        value: ë³€í™˜í•  ë¬¸ìì—´ (ì˜ˆ: "+3.5%", "-2.1%", "1,234")
        default: ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        
    Returns:
        ë³€í™˜ëœ float ê°’
    """
    if not value:
        return default
    
    try:
        # ì‰¼í‘œ, %, +, ê³µë°± ë“± ì œê±°
        cleaned = value.replace(",", "").replace("%", "").replace("+", "").strip()
        return float(cleaned)
    except (ValueError, TypeError):
        return default


def _safe_int(value: str, default: int = 0) -> int:
    """
    ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ intë¡œ ë³€í™˜
    
    Args:
        value: ë³€í™˜í•  ë¬¸ìì—´ (ì˜ˆ: "1,234ê°œ")
        default: ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        
    Returns:
        ë³€í™˜ëœ int ê°’
    """
    if not value:
        return default
    
    try:
        # ì‰¼í‘œ, 'ê°œ' ë“± ì œê±°
        cleaned = value.replace(",", "").replace("ê°œ", "").strip()
        return int(cleaned)
    except (ValueError, TypeError):
        return default


# ===== ë„¤ì´ë²„ ì¦ê¶Œ í…Œë§ˆ í¬ë¡¤ë§ =====

def crawl_naver_themes(max_pages: int = 3) -> list[dict]:
    """
    ë„¤ì´ë²„ ì¦ê¶Œ ì¸ê¸° í…Œë§ˆ í¬ë¡¤ë§
    
    ë„¤ì´ë²„ ì¦ê¶Œì˜ í…Œë§ˆ í˜ì´ì§€ì—ì„œ í…Œë§ˆ ëª©ë¡ê³¼ ë“±ë½ë¥ ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 3í˜ì´ì§€)
    
    Returns:
        í…Œë§ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸:
        [
            {
                'name': '2ì°¨ì „ì§€',
                'stock_count': 45,
                'avg_change_rate': 3.2,
                'source': 'naver',
                'url': 'https://...'
            },
            ...
        ]
    
    Example:
        >>> themes = crawl_naver_themes()
        >>> print(themes[0])
        {'name': '2ì°¨ì „ì§€', 'stock_count': 45, 'avg_change_rate': 3.2, ...}
    """
    themes = []
    base_url = "https://finance.naver.com/sise/theme.naver"
    
    logger.info("ğŸ“Š ë„¤ì´ë²„ ì¦ê¶Œ í…Œë§ˆ í¬ë¡¤ë§ ì‹œì‘")
    
    for page in range(1, max_pages + 1):
        try:
            url = f"{base_url}?&page={page}"
            
            response = httpx.get(
                url,
                headers=DEFAULT_HEADERS,
                timeout=15.0,
                follow_redirects=True
            )
            response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, "lxml")
            
            # í…Œë§ˆ í…Œì´ë¸” ì°¾ê¸°
            table = soup.find("table", class_="type_1")
            if not table:
                logger.warning(f"í…Œë§ˆ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (í˜ì´ì§€ {page})")
                continue
            
            rows = table.find_all("tr")
            
            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 5:
                    continue
                
                # í…Œë§ˆëª… ì¶”ì¶œ
                theme_link = cols[0].find("a")
                if not theme_link:
                    continue
                
                theme_name = theme_link.get_text(strip=True)
                theme_url = "https://finance.naver.com" + theme_link.get("href", "")
                
                # ì „ì¼ëŒ€ë¹„(%) ì¶”ì¶œ
                change_rate_elem = cols[1].find("span")
                change_rate = 0.0
                if change_rate_elem:
                    change_text = change_rate_elem.get_text(strip=True)
                    change_rate = _safe_float(change_text)
                    # í•˜ë½ì¸ ê²½ìš° ìŒìˆ˜ ì²˜ë¦¬
                    if "í•˜ë½" in cols[1].get_text() or "down" in str(cols[1]).lower():
                        change_rate = -abs(change_rate)
                
                # ìµœê·¼ 3ì¼ ë“±ë½ë¥  (ìˆëŠ” ê²½ìš°)
                three_day_rate = _safe_float(cols[2].get_text(strip=True)) if len(cols) > 2 else 0.0

                # ì¢…ëª© ìˆ˜ = ìƒìŠ¹(cols[3]) + ë³´í•©(cols[4]) + í•˜ë½(cols[5])
                stock_count = 0
                if len(cols) >= 6:
                    up_count = _safe_int(cols[3].get_text(strip=True))
                    flat_count = _safe_int(cols[4].get_text(strip=True))
                    down_count = _safe_int(cols[5].get_text(strip=True))
                    stock_count = up_count + flat_count + down_count

                themes.append({
                    "name": theme_name,
                    "stock_count": stock_count,
                    "avg_change_rate": change_rate,
                    "three_day_rate": three_day_rate,
                    "source": "naver",
                    "url": theme_url
                })
            
            logger.debug(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ: {len(themes)}ê°œ í…Œë§ˆ")
            _random_delay()
            
        except httpx.TimeoutException:
            logger.warning(f"ë„¤ì´ë²„ í…Œë§ˆ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (í˜ì´ì§€ {page})")
            continue
            
        except httpx.HTTPStatusError as e:
            logger.error(f"ë„¤ì´ë²„ í…Œë§ˆ HTTP ì—ëŸ¬: {e.response.status_code}")
            break
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ í…Œë§ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            continue
    
    # ì¤‘ë³µ ì œê±° (í…Œë§ˆëª… ê¸°ì¤€)
    seen = set()
    unique_themes = []
    for theme in themes:
        if theme["name"] not in seen:
            seen.add(theme["name"])
            unique_themes.append(theme)
    
    logger.info(f"âœ… ë„¤ì´ë²„ í…Œë§ˆ {len(unique_themes)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return unique_themes


def crawl_naver_theme_stocks(theme_url: str) -> list[dict]:
    """
    ë„¤ì´ë²„ ì¦ê¶Œ íŠ¹ì • í…Œë§ˆì˜ ì¢…ëª© ëª©ë¡ í¬ë¡¤ë§
    
    Args:
        theme_url: í…Œë§ˆ ìƒì„¸ í˜ì´ì§€ URL
    
    Returns:
        ì¢…ëª© ì •ë³´ ë¦¬ìŠ¤íŠ¸:
        [
            {
                'code': '005930',
                'name': 'ì‚¼ì„±ì „ì',
                'price': 75000,
                'change_rate': 2.5
            },
            ...
        ]
    """
    stocks = []
    
    try:
        response = httpx.get(
            theme_url,
            headers=DEFAULT_HEADERS,
            timeout=15.0,
            follow_redirects=True
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "lxml")
        
        # ì¢…ëª© í…Œì´ë¸” ì°¾ê¸°
        table = soup.find("table", class_="type_5")
        if not table:
            logger.warning("ì¢…ëª© í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return stocks
        
        rows = table.find_all("tr")
        
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4:
                continue
            
            # ì¢…ëª©ëª… ë° ì½”ë“œ ì¶”ì¶œ
            stock_link = cols[0].find("a")
            if not stock_link:
                continue
            
            stock_name = stock_link.get_text(strip=True)
            href = stock_link.get("href", "")
            
            # URLì—ì„œ ì¢…ëª© ì½”ë“œ ì¶”ì¶œ
            stock_code = ""
            if "code=" in href:
                import re
                match = re.search(r'code=([^&]+)', href)
                stock_code = match.group(1) if match else ""
            
            # í˜„ì¬ê°€
            price = _safe_int(cols[1].get_text(strip=True).replace(",", ""))
            
            # ë“±ë½ë¥ 
            change_rate = _safe_float(cols[2].get_text(strip=True))
            
            if stock_code:
                stocks.append({
                    "code": stock_code,
                    "name": stock_name,
                    "price": price,
                    "change_rate": change_rate
                })
        
        logger.debug(f"í…Œë§ˆ ì¢…ëª© {len(stocks)}ê°œ ìˆ˜ì§‘")
        
    except Exception as e:
        logger.error(f"í…Œë§ˆ ì¢…ëª© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    return stocks


# ===== í•œêµ­ê²½ì œ í…Œë§ˆ í¬ë¡¤ë§ =====

def crawl_hankyung_themes() -> list[dict]:
    """
    í•œêµ­ê²½ì œ ì¦ê¶Œ í…Œë§ˆ í¬ë¡¤ë§

    í•œêµ­ê²½ì œì˜ í…Œë§ˆ í˜ì´ì§€ì—ì„œ í…Œë§ˆ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Returns:
        í…Œë§ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸:
        [
            {
                'name': '2ì°¨ì „ì§€',
                'avg_change_rate': 2.5,
                'source': 'hankyung'
            },
            ...
        ]
    """
    themes = []
    urls = [
        "https://markets.hankyung.com/stock/themes",
        "https://markets.hankyung.com/theme",
    ]

    logger.info("ğŸ“Š í•œêµ­ê²½ì œ í…Œë§ˆ í¬ë¡¤ë§ ì‹œì‘")

    response = None
    for url in urls:
        try:
            response = httpx.get(
                url,
                headers=DEFAULT_HEADERS,
                timeout=15.0,
                follow_redirects=True
            )
            response.raise_for_status()
            logger.debug(f"í•œê²½ í…Œë§ˆ URL ì„±ê³µ: {url}")
            break
        except httpx.HTTPStatusError as e:
            logger.warning(f"í•œê²½ í…Œë§ˆ URL ì‹¤íŒ¨ ({e.response.status_code}): {url}")
            response = None
            continue
        except httpx.TimeoutException:
            logger.warning(f"í•œê²½ í…Œë§ˆ URL íƒ€ì„ì•„ì›ƒ: {url}")
            response = None
            continue
        except Exception as e:
            logger.warning(f"í•œê²½ í…Œë§ˆ URL ì˜¤ë¥˜: {url} - {e}")
            response = None
            continue

    if response is None:
        logger.warning("í•œêµ­ê²½ì œ í…Œë§ˆ í¬ë¡¤ë§: ëª¨ë“  URL ì‹¤íŒ¨ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
        return themes

    try:
        soup = BeautifulSoup(response.text, "lxml")

        # í…Œë§ˆ ëª©ë¡ ì°¾ê¸° (í•œê²½ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        theme_items = soup.select(".theme_item, .theme-list li, [class*='theme']")

        for item in theme_items:
            # í…Œë§ˆëª… ì¶”ì¶œ
            name_elem = item.find("a") or item.find(class_="name") or item.find("span")
            if not name_elem:
                continue

            theme_name = name_elem.get_text(strip=True)
            if not theme_name or len(theme_name) < 2:
                continue

            # ë“±ë½ë¥  ì¶”ì¶œ
            rate_elem = item.find(class_="rate") or item.find(class_="change")
            change_rate = 0.0
            if rate_elem:
                change_rate = _safe_float(rate_elem.get_text(strip=True))

            themes.append({
                "name": theme_name,
                "avg_change_rate": change_rate,
                "source": "hankyung"
            })

        logger.info(f"âœ… í•œêµ­ê²½ì œ í…Œë§ˆ {len(themes)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"í•œêµ­ê²½ì œ í…Œë§ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    return themes


# ===== ë‰´ìŠ¤ ì–¸ê¸‰ ë¹ˆë„ í¬ë¡¤ë§ =====

def crawl_theme_news_count(theme_name: str, days: int = 3) -> int:
    """
    íŠ¹ì • í…Œë§ˆì˜ ìµœê·¼ Nì¼ ë‰´ìŠ¤ ì–¸ê¸‰ íšŸìˆ˜ ì¡°íšŒ
    
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì„ í†µí•´ í…Œë§ˆ ê´€ë ¨ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    
    Args:
        theme_name: í…Œë§ˆëª… (ì˜ˆ: "2ì°¨ì „ì§€")
        days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 3ì¼)
    
    Returns:
        ë‰´ìŠ¤ ì–¸ê¸‰ íšŸìˆ˜
        
    Example:
        >>> count = crawl_theme_news_count("2ì°¨ì „ì§€", days=3)
        >>> print(count)
        127
    """
    try:
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
        # ë‚ ì§œ í•„í„°: ds (ì‹œì‘ì¼), de (ì¢…ë£Œì¼)
        today = datetime.now()
        start_date = (today - timedelta(days=days)).strftime("%Y.%m.%d")
        end_date = today.strftime("%Y.%m.%d")
        
        # ì¦ê¶Œ/ì£¼ì‹ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
        search_query = f"{theme_name} ì£¼ì‹"
        
        url = "https://search.naver.com/search.naver"
        params = {
            "where": "news",
            "query": search_query,
            "sm": "tab_opt",
            "sort": "1",  # ìµœì‹ ìˆœ
            "ds": start_date,
            "de": end_date,
            "nso": f"so:dd,p:from{start_date.replace('.', '')}to{end_date.replace('.', '')}"
        }
        
        response = httpx.get(
            url,
            params=params,
            headers=DEFAULT_HEADERS,
            timeout=10.0,
            follow_redirects=True
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "lxml")
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¶”ì¶œ
        # "ë‰´ìŠ¤ ì•½ 1,234ê±´" í˜•íƒœë¡œ í‘œì‹œë¨
        result_info = soup.find("div", class_="title_desc")
        if result_info:
            text = result_info.get_text()
            # ìˆ«ì ì¶”ì¶œ
            import re
            numbers = re.findall(r"[\d,]+", text)
            if numbers:
                count = _safe_int(numbers[0])
                logger.debug(f"[{theme_name}] ë‰´ìŠ¤ {count}ê±´")
                return count
        
        # ë‰´ìŠ¤ ì•„ì´í…œ ìˆ˜ë¡œ ëŒ€ëµ ì¶”ì • (ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°)
        news_items = soup.select(".news_area, .bx, .list_news li")
        count = len(news_items)
        
        logger.debug(f"[{theme_name}] ë‰´ìŠ¤ ì•½ {count}ê±´ (ì¶”ì •)")
        return count
        
    except Exception as e:
        logger.warning(f"[{theme_name}] ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return 0


def crawl_multiple_theme_news(theme_names: list[str], days: int = 3) -> dict[str, int]:
    """
    ì—¬ëŸ¬ í…Œë§ˆì˜ ë‰´ìŠ¤ ì–¸ê¸‰ íšŸìˆ˜ ì¼ê´„ ì¡°íšŒ
    
    Args:
        theme_names: í…Œë§ˆëª… ë¦¬ìŠ¤íŠ¸
        days: ì¡°íšŒí•  ì¼ìˆ˜
    
    Returns:
        {í…Œë§ˆëª…: ë‰´ìŠ¤ ìˆ˜} ë”•ì…”ë„ˆë¦¬
        
    Example:
        >>> counts = crawl_multiple_theme_news(["2ì°¨ì „ì§€", "AIë°˜ë„ì²´"])
        >>> print(counts)
        {'2ì°¨ì „ì§€': 127, 'AIë°˜ë„ì²´': 95}
    """
    results = {}
    
    logger.info(f"ğŸ“° {len(theme_names)}ê°œ í…Œë§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    
    for theme_name in theme_names:
        count = crawl_theme_news_count(theme_name, days)
        results[theme_name] = count
        _random_delay()  # ì°¨ë‹¨ ë°©ì§€
    
    logger.info(f"âœ… í…Œë§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    return results


# ===== ìì²´ ì •ì˜ í…Œë§ˆ ëª©ë¡ =====

def get_predefined_themes() -> list[dict]:
    """
    ìì²´ ì •ì˜ëœ 20ê°œ í•µì‹¬ í…Œë§ˆ ë°˜í™˜

    ë„¤ì´ë²„/í•œê²½ì— ì—†ê±°ë‚˜ ì¤‘ìš”í•œ í…Œë§ˆë¥¼ ì§ì ‘ ì •ì˜í•©ë‹ˆë‹¤.

    Returns:
        í…Œë§ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    predefined = [
        {"name": "2ì°¨ì „ì§€", "category": "ì‹ ì„±ì¥", "keywords": ["ë°°í„°ë¦¬", "ë¦¬íŠ¬", "ì „ê¸°ì°¨"]},
        {"name": "AIë°˜ë„ì²´", "category": "ë°˜ë„ì²´", "keywords": ["AIì¹©", "HBM", "GPU"]},
        {"name": "ë°˜ë„ì²´", "category": "ë°˜ë„ì²´", "keywords": ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "íŒŒìš´ë“œë¦¬"]},
        {"name": "K-ë°©ì‚°", "category": "ë°©ìœ„ì‚°ì—…", "keywords": ["ë°©ì‚°", "ë¬´ê¸°", "ìˆ˜ì¶œ"]},
        {"name": "ë°”ì´ì˜¤", "category": "í—¬ìŠ¤ì¼€ì–´", "keywords": ["ì‹ ì•½", "ì„ìƒ", "ë°”ì´ì˜¤í…"]},
        {"name": "ë¡œë´‡", "category": "ì‹ ì„±ì¥", "keywords": ["ë¡œë´‡", "ìë™í™”", "íœ´ë¨¸ë…¸ì´ë“œ"]},
        {"name": "ììœ¨ì£¼í–‰", "category": "ëª¨ë¹Œë¦¬í‹°", "keywords": ["ììœ¨ì£¼í–‰", "ë¼ì´ë‹¤", "ì„¼ì„œ"]},
        {"name": "ì›ìë ¥", "category": "ì—ë„ˆì§€", "keywords": ["ì›ì „", "SMR", "í•µìœµí•©"]},
        {"name": "ìˆ˜ì†Œ", "category": "ì—ë„ˆì§€", "keywords": ["ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ"]},
        {"name": "ì¡°ì„ ", "category": "ì‚°ì—…ì¬", "keywords": ["ì¡°ì„ ", "LNGì„ ", "ì»¨í…Œì´ë„ˆì„ "]},
        {"name": "ê±´ì„¤", "category": "ì‚°ì—…ì¬", "keywords": ["ê±´ì„¤", "ì•„íŒŒíŠ¸", "ì¸í”„ë¼"]},
        {"name": "ê¸ˆìœµ", "category": "ê¸ˆìœµ", "keywords": ["ì€í–‰", "ì¦ê¶Œ", "ë³´í—˜"]},
        {"name": "ì—”í„°í…Œì¸ë¨¼íŠ¸", "category": "ì†Œë¹„ì¬", "keywords": ["K-POP", "ë“œë¼ë§ˆ", "ì½˜í…ì¸ "]},
        {"name": "ê²Œì„", "category": "ITì„œë¹„ìŠ¤", "keywords": ["ê²Œì„", "ëª¨ë°”ì¼ê²Œì„", "eìŠ¤í¬ì¸ "]},
        {"name": "í”Œë«í¼", "category": "ITì„œë¹„ìŠ¤", "keywords": ["í”Œë«í¼", "ì´ì»¤ë¨¸ìŠ¤", "í•€í…Œí¬"]},
        {"name": "í´ë¼ìš°ë“œ", "category": "ITì„œë¹„ìŠ¤", "keywords": ["í´ë¼ìš°ë“œ", "SaaS", "ë°ì´í„°ì„¼í„°"]},
        {"name": "ìŒì‹ë£Œ", "category": "ì†Œë¹„ì¬", "keywords": ["ì‹í’ˆ", "ìŒë£Œ", "ì£¼ë¥˜"]},
        {"name": "í™”ì¥í’ˆ", "category": "ì†Œë¹„ì¬", "keywords": ["í™”ì¥í’ˆ", "K-ë·°í‹°", "ìŠ¤í‚¨ì¼€ì–´"]},
        {"name": "ì² ê°•", "category": "ì†Œì¬", "keywords": ["ì² ê°•", "ìŠ¤í…Œì¸ë¦¬ìŠ¤", "íŠ¹ìˆ˜ê°•"]},
        {"name": "í™”í•™", "category": "ì†Œì¬", "keywords": ["í™”í•™", "ì„ìœ í™”í•™", "ì •ë°€í™”í•™"]},
        {"name": "í†µì‹ ", "category": "í†µì‹ ", "keywords": ["5G", "6G", "í†µì‹ ì¥ë¹„"]},
    ]

    return [
        {
            "name": t["name"],
            "category": t["category"],
            "keywords": t["keywords"],
            "source": "predefined"
        }
        for t in predefined
    ]


def search_naver_theme(theme_name: str) -> Optional[dict]:
    """
    ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ íŠ¹ì • í…Œë§ˆë¥¼ ê²€ìƒ‰í•˜ì—¬ ë°ì´í„° ë°˜í™˜

    predefined í…Œë§ˆê°€ ì¼ë°˜ í¬ë¡¤ë§ì—ì„œ ëˆ„ë½ëœ ê²½ìš° ì§ì ‘ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        theme_name: ê²€ìƒ‰í•  í…Œë§ˆëª…

    Returns:
        í…Œë§ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        # ë„¤ì´ë²„ í…Œë§ˆ ê²€ìƒ‰ URL (í…Œë§ˆëª…ìœ¼ë¡œ ê²€ìƒ‰)
        search_url = "https://finance.naver.com/sise/theme.naver"

        response = httpx.get(
            search_url,
            headers=DEFAULT_HEADERS,
            timeout=10.0,
            follow_redirects=True
        )
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "lxml")
        table = soup.find("table", class_="type_1")

        if not table:
            return None

        rows = table.find_all("tr")

        # í…Œë§ˆëª…ê³¼ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸° (ë¶€ë¶„ ë§¤ì¹­)
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5:
                continue

            theme_link = cols[0].find("a")
            if not theme_link:
                continue

            found_name = theme_link.get_text(strip=True)

            # ë¶€ë¶„ ë§¤ì¹­ (ì˜ˆ: "2ì°¨ì „ì§€" in "2ì°¨ì „ì§€ ê´€ë ¨ì£¼")
            if theme_name in found_name or found_name in theme_name:
                theme_url = "https://finance.naver.com" + theme_link.get("href", "")

                # ë“±ë½ë¥  ì¶”ì¶œ
                change_rate_elem = cols[1].find("span")
                change_rate = 0.0
                if change_rate_elem:
                    change_text = change_rate_elem.get_text(strip=True)
                    change_rate = _safe_float(change_text)
                    if "í•˜ë½" in cols[1].get_text() or "down" in str(cols[1]).lower():
                        change_rate = -abs(change_rate)

                # 3ì¼ ë“±ë½ë¥ 
                three_day_rate = _safe_float(cols[2].get_text(strip=True)) if len(cols) > 2 else 0.0

                # ì¢…ëª© ìˆ˜ = ìƒìŠ¹ + ë³´í•© + í•˜ë½
                stock_count = 0
                if len(cols) >= 6:
                    stock_count = _safe_int(cols[3].get_text(strip=True)) + _safe_int(cols[4].get_text(strip=True)) + _safe_int(cols[5].get_text(strip=True))

                logger.debug(f"[{theme_name}] ë„¤ì´ë²„ì—ì„œ ë°œê²¬: {found_name} ({change_rate:+.2f}%, {stock_count}ì¢…ëª©)")

                return {
                    "name": theme_name,  # ì›ë˜ ê²€ìƒ‰í•œ ì´ë¦„ ìœ ì§€
                    "naver_name": found_name,
                    "stock_count": stock_count,
                    "avg_change_rate": change_rate,
                    "three_day_rate": three_day_rate,
                    "source": "naver_search",
                    "url": theme_url
                }

        # ì „ì²´ í˜ì´ì§€ ê²€ìƒ‰ (ìµœëŒ€ 5í˜ì´ì§€)
        for page in range(2, 6):
            _random_delay()

            page_url = f"{search_url}?&page={page}"
            response = httpx.get(page_url, headers=DEFAULT_HEADERS, timeout=10.0, follow_redirects=True)
            soup = BeautifulSoup(response.text, "lxml")
            table = soup.find("table", class_="type_1")

            if not table:
                continue

            rows = table.find_all("tr")

            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 5:
                    continue

                theme_link = cols[0].find("a")
                if not theme_link:
                    continue

                found_name = theme_link.get_text(strip=True)

                if theme_name in found_name or found_name in theme_name:
                    theme_url = "https://finance.naver.com" + theme_link.get("href", "")

                    change_rate_elem = cols[1].find("span")
                    change_rate = 0.0
                    if change_rate_elem:
                        change_text = change_rate_elem.get_text(strip=True)
                        change_rate = _safe_float(change_text)
                        if "í•˜ë½" in cols[1].get_text() or "down" in str(cols[1]).lower():
                            change_rate = -abs(change_rate)

                    three_day_rate = _safe_float(cols[2].get_text(strip=True)) if len(cols) > 2 else 0.0

                    # ì¢…ëª© ìˆ˜ = ìƒìŠ¹ + ë³´í•© + í•˜ë½
                    stock_count = 0
                    if len(cols) >= 6:
                        stock_count = _safe_int(cols[3].get_text(strip=True)) + _safe_int(cols[4].get_text(strip=True)) + _safe_int(cols[5].get_text(strip=True))

                    logger.debug(f"[{theme_name}] ë„¤ì´ë²„ í˜ì´ì§€{page}ì—ì„œ ë°œê²¬: {found_name} ({change_rate:+.2f}%, {stock_count}ì¢…ëª©)")

                    return {
                        "name": theme_name,
                        "naver_name": found_name,
                        "stock_count": stock_count,
                        "avg_change_rate": change_rate,
                        "three_day_rate": three_day_rate,
                        "source": "naver_search",
                        "url": theme_url
                    }

        logger.debug(f"[{theme_name}] ë„¤ì´ë²„ì—ì„œ ì°¾ì§€ ëª»í•¨")
        return None

    except Exception as e:
        logger.warning(f"[{theme_name}] ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return None


# ===== í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ =====

def crawl_all_themes() -> list[dict]:
    """
    ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í…Œë§ˆ ë°ì´í„° í†µí•© ìˆ˜ì§‘

    ë„¤ì´ë²„, í•œê²½, ìì²´ì •ì˜ í…Œë§ˆë¥¼ ëª¨ë‘ ìˆ˜ì§‘í•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤.
    predefined í…Œë§ˆëŠ” ë„¤ì´ë²„ì—ì„œ ì‹¤ì œ ì‹œì¥ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Returns:
        í†µí•©ëœ í…Œë§ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸

    Example:
        >>> all_themes = crawl_all_themes()
        >>> print(f"ì´ {len(all_themes)}ê°œ í…Œë§ˆ ìˆ˜ì§‘")
    """
    all_themes = []

    logger.info("ğŸ”„ ì „ì²´ í…Œë§ˆ í¬ë¡¤ë§ ì‹œì‘")

    # 1. ë„¤ì´ë²„ í…Œë§ˆ (ìµœëŒ€ 5í˜ì´ì§€ë¡œ í™•ì¥)
    try:
        naver_themes = crawl_naver_themes(max_pages=5)
        all_themes.extend(naver_themes)
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ í…Œë§ˆ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    _random_delay()

    # 2. í•œê²½ í…Œë§ˆ
    try:
        hankyung_themes = crawl_hankyung_themes()
        all_themes.extend(hankyung_themes)
    except Exception as e:
        logger.error(f"í•œê²½ í…Œë§ˆ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # í˜„ì¬ ìˆ˜ì§‘ëœ í…Œë§ˆëª… ëª©ë¡
    collected_names = {t["name"] for t in all_themes}

    # 3. ìì²´ ì •ì˜ í…Œë§ˆ - ë„¤ì´ë²„ì—ì„œ ì‹¤ì œ ë°ì´í„° ì¡°íšŒ
    predefined_themes = get_predefined_themes()
    enriched_count = 0

    logger.info(f"ğŸ“Š ì£¼ìš” í…Œë§ˆ {len(predefined_themes)}ê°œ ë°ì´í„° ë³´ê°• ì¤‘...")

    for predef in predefined_themes:
        theme_name = predef["name"]

        # ì´ë¯¸ ìˆ˜ì§‘ëœ í…Œë§ˆë©´ ìŠ¤í‚µ (ì‹¤ì œ ë°ì´í„°ê°€ ìˆìŒ)
        if theme_name in collected_names:
            logger.debug(f"[{theme_name}] ì´ë¯¸ ìˆ˜ì§‘ë¨ - ìŠ¤í‚µ")
            continue

        # ë„¤ì´ë²„ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        _random_delay()
        naver_data = search_naver_theme(theme_name)

        if naver_data:
            # ë„¤ì´ë²„ì—ì„œ ì°¾ì€ ë°ì´í„°ì™€ predefined ì •ë³´ ë³‘í•©
            enriched_theme = {
                **predef,
                **naver_data,
                "category": predef.get("category", "ê¸°íƒ€"),
                "keywords": predef.get("keywords", []),
            }
            all_themes.append(enriched_theme)
            enriched_count += 1
            logger.info(f"  âœ“ [{theme_name}] ë°ì´í„° ë³´ê°• ì™„ë£Œ: {naver_data.get('avg_change_rate', 0):+.2f}%, {naver_data.get('stock_count', 0)}ì¢…ëª©")
        else:
            # ë„¤ì´ë²„ì—ì„œ ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€ (ëª¨ë©˜í…€ 0ì ì´ì§€ë§Œ í¬í•¨)
            predef_with_defaults = {
                **predef,
                "avg_change_rate": 0.0,
                "stock_count": 15,  # ì£¼ìš” í…Œë§ˆëŠ” ìµœì†Œ 15ì¢…ëª© ê°€ì •
                "source": "predefined_default"
            }
            all_themes.append(predef_with_defaults)
            logger.warning(f"  âœ— [{theme_name}] ë„¤ì´ë²„ ë¯¸ë°œê²¬ - ê¸°ë³¸ê°’ ì‚¬ìš©")

    logger.info(f"ğŸ“Š ì£¼ìš” í…Œë§ˆ {enriched_count}ê°œ ë°ì´í„° ë³´ê°• ì™„ë£Œ")

    # ì¤‘ë³µ ì œê±° (í…Œë§ˆëª… ê¸°ì¤€, ì²« ë²ˆì§¸ ê²ƒ ìœ ì§€)
    seen = set()
    unique_themes = []
    for theme in all_themes:
        if theme["name"] not in seen:
            seen.add(theme["name"])
            unique_themes.append(theme)

    logger.info(f"âœ… ì „ì²´ í…Œë§ˆ {len(unique_themes)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    return unique_themes


# ===== ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ =====
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ” í…Œë§ˆ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ë„¤ì´ë²„ í…Œë§ˆ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š ë„¤ì´ë²„ í…Œë§ˆ í¬ë¡¤ë§...")
    naver_themes = crawl_naver_themes(max_pages=1)
    print(f"ìˆ˜ì§‘ëœ í…Œë§ˆ: {len(naver_themes)}ê°œ")
    for theme in naver_themes[:5]:
        print(f"  - {theme['name']}: {theme['avg_change_rate']:+.2f}%")
    
    # ë‰´ìŠ¤ ì¹´ìš´íŠ¸ í…ŒìŠ¤íŠ¸
    if naver_themes:
        print("\nğŸ“° ë‰´ìŠ¤ ì¹´ìš´íŠ¸ í…ŒìŠ¤íŠ¸...")
        test_theme = naver_themes[0]["name"]
        news_count = crawl_theme_news_count(test_theme, days=3)
        print(f"  {test_theme}: {news_count}ê±´")
    
    # ìì²´ ì •ì˜ í…Œë§ˆ
    print("\nğŸ“‹ ìì²´ ì •ì˜ í…Œë§ˆ...")
    predefined = get_predefined_themes()
    print(f"ì •ì˜ëœ í…Œë§ˆ: {len(predefined)}ê°œ")
    for theme in predefined[:5]:
        print(f"  - {theme['name']} ({theme['category']})")
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
