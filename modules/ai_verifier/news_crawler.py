"""
news_crawler.py - ë‰´ìŠ¤ í¬ë¡¤ë§ ëª¨ë“ˆ

ì´ íŒŒì¼ì€ ì¢…ëª©ë³„ ìµœê·¼ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§
- ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
- ë‰´ìŠ¤ ìš”ì•½ ìƒì„±

ì‚¬ìš©ë²•:
    from modules.ai_verifier.news_crawler import (
        fetch_stock_news,
        fetch_multiple_stocks_news
    )
    
    news = fetch_stock_news("005930", days=7)
"""

import time
import random
from datetime import datetime, timedelta
from typing import Optional

import httpx
from bs4 import BeautifulSoup

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from logger import logger
from config import now_kst


# ===== ìƒìˆ˜ ì •ì˜ =====
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8",
    "Referer": "https://finance.naver.com/"
}

MIN_DELAY = 0.5
MAX_DELAY = 1.5


def _random_delay():
    """ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ëŒ€ê¸°"""
    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))


def fetch_stock_news(
    stock_code: str,
    days: int = 7,
    max_articles: int = 10
) -> list[dict]:
    """
    ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë„¤ì´ë²„ ê¸ˆìœµ)
    
    Args:
        stock_code: ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930")
        days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
        max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸:
        [
            {
                'title': 'ì‚¼ì„±ì „ì, ë°˜ë„ì²´ íˆ¬ì í™•ëŒ€...',
                'summary': 'ì‚¼ì„±ì „ìê°€ ë°˜ë„ì²´ ì‹œì„¤ íˆ¬ìë¥¼...',
                'date': '2024-02-01',
                'source': 'í•œêµ­ê²½ì œ',
                'url': 'https://...'
            },
            ...
        ]
        
    Example:
        >>> news = fetch_stock_news("005930", days=7)
        >>> print(news[0]['title'])
        'ì‚¼ì„±ì „ì, AI ë°˜ë„ì²´ íˆ¬ì í™•ëŒ€ ë°œí‘œ'
    """
    news_list = []
    
    # ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© ë‰´ìŠ¤ URL
    url = f"https://finance.naver.com/item/news_news.naver"
    
    params = {
        "code": stock_code,
        "page": 1,
        "sm": "title_entity_id.basic",
        "clusterId": ""
    }
    
    logger.debug(f"[{stock_code}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼)")
    
    try:
        page = 1
        cutoff_date = now_kst() - timedelta(days=days)
        
        while len(news_list) < max_articles and page <= 5:
            params["page"] = page
            
            response = httpx.get(
                url,
                params=params,
                headers=DEFAULT_HEADERS,
                timeout=15.0,
                follow_redirects=True
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "lxml")
            
            # ë‰´ìŠ¤ í…Œì´ë¸” ì°¾ê¸°
            news_table = soup.find("table", class_="type5")
            if not news_table:
                break
            
            rows = news_table.find_all("tr")
            
            found_old = False
            for row in rows:
                # ì œëª© ì¶”ì¶œ
                title_elem = row.find("a", class_="tit")
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                news_url = "https://finance.naver.com" + title_elem.get("href", "")
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_elem = row.find("td", class_="date")
                if date_elem:
                    date_str = date_elem.get_text(strip=True)
                    try:
                        news_date = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                    except ValueError:
                        news_date = now_kst()
                else:
                    news_date = now_kst()
                
                # ê¸°ê°„ ì²´í¬
                if news_date < cutoff_date:
                    found_old = True
                    break
                
                # ì¶œì²˜ ì¶”ì¶œ
                source_elem = row.find("td", class_="info")
                source = source_elem.get_text(strip=True) if source_elem else "Unknown"
                
                news_list.append({
                    "title": title,
                    "summary": "",  # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ í•„ìš”
                    "date": news_date.strftime("%Y-%m-%d"),
                    "source": source,
                    "url": news_url
                })
                
                if len(news_list) >= max_articles:
                    break
            
            if found_old:
                break
            
            page += 1
            _random_delay()
        
        logger.info(f"[{stock_code}] ë‰´ìŠ¤ {len(news_list)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"[{stock_code}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    return news_list


def fetch_news_content(news_url: str) -> Optional[str]:
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
    
    Args:
        news_url: ë‰´ìŠ¤ URL
    
    Returns:
        ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ (ìµœëŒ€ 2000ì)
    """
    try:
        response = httpx.get(
            news_url,
            headers=DEFAULT_HEADERS,
            timeout=10.0,
            follow_redirects=True
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "lxml")
        
        # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_elem = soup.find("div", id="news_read") or soup.find("div", class_="article_body")
        
        if content_elem:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in content_elem.find_all(["script", "style", "iframe"]):
                tag.decompose()
            
            text = content_elem.get_text(separator=" ", strip=True)
            
            # ê¸¸ì´ ì œí•œ
            if len(text) > 2000:
                text = text[:2000] + "..."
            
            return text
        
        return None
        
    except Exception as e:
        logger.warning(f"ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def fetch_stock_news_with_content(
    stock_code: str,
    days: int = 7,
    max_articles: int = 5
) -> list[dict]:
    """
    ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ (ë³¸ë¬¸ í¬í•¨)
    
    Args:
        stock_code: ì¢…ëª©ì½”ë“œ
        days: ìˆ˜ì§‘ ê¸°ê°„
        max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
    
    Returns:
        ë³¸ë¬¸ì´ í¬í•¨ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    news_list = fetch_stock_news(stock_code, days, max_articles)
    
    for news in news_list:
        if news.get("url"):
            content = fetch_news_content(news["url"])
            if content:
                news["content"] = content
                news["summary"] = content[:200] + "..." if len(content) > 200 else content
            _random_delay()
    
    return news_list


def fetch_multiple_stocks_news(
    stock_codes: list[str],
    days: int = 7,
    max_per_stock: int = 5
) -> dict[str, list[dict]]:
    """
    ì—¬ëŸ¬ ì¢…ëª©ì˜ ë‰´ìŠ¤ ì¼ê´„ ìˆ˜ì§‘
    
    Args:
        stock_codes: ì¢…ëª©ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        days: ìˆ˜ì§‘ ê¸°ê°„
        max_per_stock: ì¢…ëª©ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
    
    Returns:
        {ì¢…ëª©ì½”ë“œ: [ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸]} ë”•ì…”ë„ˆë¦¬
        
    Example:
        >>> news_dict = fetch_multiple_stocks_news(["005930", "000660"])
        >>> print(len(news_dict["005930"]))
        5
    """
    logger.info(f"ğŸ“° {len(stock_codes)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    
    result = {}
    
    for code in stock_codes:
        news = fetch_stock_news(code, days, max_per_stock)
        result[code] = news
        _random_delay()
    
    total = sum(len(v) for v in result.values())
    logger.info(f"âœ… ì´ {total}ê±´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return result


def format_news_for_ai(news_list: list[dict]) -> str:
    """
    ë‰´ìŠ¤ë¥¼ AI ë¶„ì„ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
    
    Args:
        news_list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
    """
    if not news_list:
        return "ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    lines = []
    
    for i, news in enumerate(news_list, 1):
        lines.append(f"[ë‰´ìŠ¤ {i}] ({news.get('date', 'ë‚ ì§œ ë¯¸ìƒ')})")
        lines.append(f"ì œëª©: {news.get('title', '')}")
        
        content = news.get("content") or news.get("summary", "")
        if content:
            lines.append(f"ë‚´ìš©: {content}")
        
        lines.append("")
    
    return "\n".join(lines)


# ===== ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ =====
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‚¼ì„±ì „ì ë‰´ìŠ¤ í…ŒìŠ¤íŠ¸
    test_code = "005930"
    print(f"\nì¢…ëª©: {test_code}")
    print("-" * 40)
    
    news = fetch_stock_news(test_code, days=7, max_articles=5)
    
    print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news)}ê±´")
    for n in news[:3]:
        print(f"  - [{n['date']}] {n['title'][:40]}...")
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
